# ðŸŒŒ AxialTransformer-Compact â€” Autoregressive 2D Image Modeling

This repository provides a **PyTorch implementation** of the **Axial-Transformer**  
for **autoregressive image modeling**, faithfully replicating the paperâ€™s  
architecture, mathematics, and block structure ðŸª.

The focus is **on code clarity and reproducibility**, not on training or benchmarks.  
The model implements:
- Factorized **axial attention** (row + column) ðŸ§¬
- **Masked causal attention** for autoregressive sampling ðŸ”’
- Integration of **positional embeddings** with horizontal and vertical contexts ðŸ§©

**Paper reference:**  
[Axial-Deep Learning for Autoregressive Image Modeling](https://arxiv.org/abs/1912.12180) ðŸŒ 

---

## ðŸ”­ Overview â€” Long-Range Spatial Dependency

![AxialTransformer Overview](images/figmix.jpg)

> Convolutional networks struggle with **global dependencies** in 2D images.  
> Axial-Transformer addresses this by **factorizing attention** along rows and columns,  
> allowing each pixel to attend efficiently to all previous pixels while preserving **causality**.

Highlights:

- **Convolutions** capture local structure
- **Axial attention** models global, long-range interactions
- **Masked attention** ensures $$p(x) = \prod_i p(x_i | x_{<i})$$ causality
- **Positional embeddings** provide spatial context along both axes

---

## ðŸ§® Attention Formulation

For an input tensor  

$$
x \in \mathbb{R}^{H \times W \times D}
$$

the **row** and **column** attention computations produce contexts:

$$
u = \text{AxialAttention}_{\text{row+col}}(x_{\text{above}})
$$

$$
h = \text{MaskedRowAttention}(x_{\text{current row, left}})
$$

These are combined with positional embeddings:

$$
h_i = h_i + u_i + \text{pos}_i
$$

Finally, the logits over pixel values are produced via a linear layer:

$$
\text{logits}_i = W_o h_i
$$

This **factorized approach** allows the model to scale to high-resolution images efficiently.

---

## âš™ Architectural Principle

- **Pixel embeddings** map discrete pixel values into $$D$$-dimensional vectors
- **Axial blocks** alternate row and column attention with feedforward layers
- **Upper-context computation** handles information from previous rows
- **Decoder stack** combines masked row attention with upper-context to enforce causality
- The model can compute **exact log-likelihoods** autoregressively

---

## ðŸ§© Repository Structure

```bash
AxialTransformer-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ masks.py             # row / column causal masks
â”‚   â”‚   â””â”€â”€ visualization.py     # attention & receptive field gÃ¶rselleri
â”‚   â”‚
â”‚   â”œâ”€â”€ attention/
â”‚   â”‚   â””â”€â”€ attention.py         # AxialAttention_k + MaskedAttention_k
â”‚   â”‚
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â””â”€â”€ layers.py            # shift_right, shift_down, embeddings, FFN
â”‚   â”‚
â”‚   â”œâ”€â”€ blocks/
â”‚   â”‚   â””â”€â”€ blocks.py            # AttentionBlock + TransformerBlock
â”‚   â”‚
â”‚   â”œâ”€â”€ decoder/
â”‚   â”‚   â””â”€â”€ decoder.py           # u compute + h compute + h = h+u+pos
â”‚   â”‚
â”‚   â”‚
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ model.py            
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ðŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
